# Instruction Tuning LLMs for Data Annotation

## Could ChatGPT 3.5 and LLaMA 2 be used as annotators?

If they can understand the principle of helpfulness, follow annotation guideline and generate output annotation in the correct format. It could act as a copilot for annotators, saving major time and headache for the annota- tor. No more copy paste and fact checking! 

In this work, a small set of instruction following data with the correct annotation is used to instruction tuning for LLMs. **ChatGPT 3.5** would work as a timely reliable assistant, but **Llama 2** is not ideal as it is slow to learning, response and not as reliable.

---

### Previous Project:
[Breaking Down Helpfulness](https://github.com/RL4LMT/Breaking-down-Helpfulness)

